# CHUNK

        lsl x11, x11, #22                      // ........................................................................................................................................................................................................................*...................................................
        add x14, x14, x7                       // ........................................................................................................................................................................................................................*...................................................
        mov x15, x12                           // .........................................................................................................................................................................................................................*..................................................
        add x12, x12, #1048576                 // .........................................................................................................................................................................................................................*..................................................
        and v14.16B, v19.16B, v1.16B           // ..........................................................................................................................................................................................................................*.................................................
        sshr v19.2D, v19.2D, #30               // ..........................................................................................................................................................................................................................*.................................................
        add x15, x15, #1048576                 // ..........................................................................................................................................................................................................................*.................................................
        asr x11, x11, #43                      // ...........................................................................................................................................................................................................................*................................................
        asr x14, x14, #42                      // ...........................................................................................................................................................................................................................*................................................
        lsl x12, x12, #22                      // ...........................................................................................................................................................................................................................*................................................
        add x15, x15, x7                       // ............................................................................................................................................................................................................................*...............................................
        smlal v19.2D, v18.2S, v15.S[0]         // .............................................................................................................................................................................................................................*..............................................
        asr x12, x12, #43                      // .............................................................................................................................................................................................................................*..............................................
        smlal2 v19.2D, v18.4S, v15.S[2]        // ..............................................................................................................................................................................................................................*.............................................
        asr x15, x15, #42                      // ..............................................................................................................................................................................................................................*.............................................
        add v19.2D, v19.2D, v24.2D             // ..................................................................................................................................................................................................................................*.........................................
        and v15.16B, v19.16B, v1.16B           // .....................................................................................................................................................................................................................................*......................................
        shl v15.2D, v15.2D, #32                // ........................................................................................................................................................................................................................................*...................................
        orr v14.16B, v14.16B, v15.16B          // ...........................................................................................................................................................................................................................................*................................
        sshr v15.2D, v19.2D, #30               // ...........................................................................................................................................................................................................................................*................................
        sshr v16.2D, v15.2D, #15               // ..............................................................................................................................................................................................................................................*.............................
        and v15.16B, v15.16B, v2.16B           // ..............................................................................................................................................................................................................................................*.............................
        mul v19.4S, v5.4S, v16.4S              // .................................................................................................................................................................................................................................................*..........................
        and v19.16B, v19.16B, v0.16B           // ......................................................................................................................................................................................................................................................*.....................
        add v11.4S, v11.4S, v19.4S             // .........................................................................................................................................................................................................................................................*..................
        sshr v16.4S, v11.4S, #30               // ............................................................................................................................................................................................................................................................*...............
        shl v16.2D, v16.2D, #32                // ...............................................................................................................................................................................................................................................................*............
        add v11.4S, v11.4S, v16.4S             // ..................................................................................................................................................................................................................................................................*.........
        sshr v16.4S, v11.4S, #30               // .....................................................................................................................................................................................................................................................................*......
        and v11.16B, v11.16B, v3.16B           // .....................................................................................................................................................................................................................................................................*......
        ushr v16.2D, v16.2D, #32               // ........................................................................................................................................................................................................................................................................*...
        add v12.4S, v12.4S, v16.4S             // ...........................................................................................................................................................................................................................................................................*



# qhasm: int64 tmp

# qhasm: int64 prod_lo

# qhasm: int64 prod_hi

# qhasm: int64 new_f

# qhasm: int64 new_g

# qhasm: int64 new_uu

# qhasm: int64 new_vv

# qhasm: int64 new_rr

# qhasm: int64 new_ss

# qhasm: prod_lo = uu * f
# asm 1: mul >prod_lo=int64#17,<uu=int64#12,<f=int64#10
# asm 2: mul >prod_lo=x16,<uu=x11,<f=x9
mul x16,x11,x9

# qhasm: prod_hi = uu signed* f (hi)
# asm 1: smulh >prod_hi=int64#18, <uu=int64#12, <f=int64#10
# asm 2: smulh >prod_hi=x17, <uu=x11, <f=x9
smulh x17, x11, x9

# qhasm: tmp = vv * g
# asm 1: mul >tmp=int64#19,<vv=int64#15,<g=int64#11
# asm 2: mul >tmp=x18,<vv=x14,<g=x10
mul x18,x14,x10

# qhasm: prod_lo += tmp !
# asm 1: adds <prod_lo=int64#17, <prod_lo=int64#17, <tmp=int64#19
# asm 2: adds <prod_lo=x16, <prod_lo=x16, <tmp=x18
adds x16, x16, x18

# qhasm: tmp = vv signed* g (hi)
# asm 1: smulh >tmp=int64#19, <vv=int64#15, <g=int64#11
# asm 2: smulh >tmp=x18, <vv=x14, <g=x10
smulh x18, x14, x10

# qhasm: prod_hi = prod_hi + tmp + carry 
# asm 1: adc >prod_hi=int64#18,<prod_hi=int64#18,<tmp=int64#19
# asm 2: adc >prod_hi=x17,<prod_hi=x17,<tmp=x18
adc x17,x17,x18

# qhasm: prod_lo = prod_lo unsigned>> 20
# asm 1: lsr >prod_lo=int64#17, <prod_lo=int64#17, #20
# asm 2: lsr >prod_lo=x16, <prod_lo=x16, #20
lsr x16, x16, #20

# qhasm: prod_hi = prod_hi << 44
# asm 1: lsl >prod_hi=int64#18, <prod_hi=int64#18, #44
# asm 2: lsl >prod_hi=x17, <prod_hi=x17, #44
lsl x17, x17, #44

# qhasm: new_f = prod_lo | prod_hi
# asm 1: orr >new_f=int64#17, <prod_lo=int64#17, <prod_hi=int64#18
# asm 2: orr >new_f=x16, <prod_lo=x16, <prod_hi=x17
orr x16, x16, x17

# qhasm: prod_lo = rr * f
# asm 1: mul >prod_lo=int64#18,<rr=int64#13,<f=int64#10
# asm 2: mul >prod_lo=x17,<rr=x12,<f=x9
mul x17,x12,x9

# qhasm: prod_hi = rr signed* f (hi)
# asm 1: smulh >prod_hi=int64#10, <rr=int64#13, <f=int64#10
# asm 2: smulh >prod_hi=x9, <rr=x12, <f=x9
smulh x9, x12, x9

# qhasm: tmp = ss * g
# asm 1: mul >tmp=int64#19,<ss=int64#16,<g=int64#11
# asm 2: mul >tmp=x18,<ss=x15,<g=x10
mul x18,x15,x10

# qhasm: prod_lo += tmp !
# asm 1: adds <prod_lo=int64#18, <prod_lo=int64#18, <tmp=int64#19
# asm 2: adds <prod_lo=x17, <prod_lo=x17, <tmp=x18
adds x17, x17, x18

# qhasm: tmp = ss signed* g (hi)
# asm 1: smulh >tmp=int64#11, <ss=int64#16, <g=int64#11
# asm 2: smulh >tmp=x10, <ss=x15, <g=x10
smulh x10, x15, x10

# qhasm: prod_hi = prod_hi + tmp + carry 
# asm 1: adc >prod_hi=int64#10,<prod_hi=int64#10,<tmp=int64#11
# asm 2: adc >prod_hi=x9,<prod_hi=x9,<tmp=x10
adc x9,x9,x10

# qhasm: prod_lo = prod_lo unsigned>> 20
# asm 1: lsr >prod_lo=int64#11, <prod_lo=int64#18, #20
# asm 2: lsr >prod_lo=x10, <prod_lo=x17, #20
lsr x10, x17, #20

# qhasm: prod_hi = prod_hi << 44
# asm 1: lsl >prod_hi=int64#10, <prod_hi=int64#10, #44
# asm 2: lsl >prod_hi=x9, <prod_hi=x9, #44
lsl x9, x9, #44

# qhasm: g = prod_lo | prod_hi
# asm 1: orr >g=int64#10, <prod_lo=int64#11, <prod_hi=int64#10
# asm 2: orr >g=x9, <prod_lo=x10, <prod_hi=x9
orr x9, x10, x9

# qhasm: f = new_f
# asm 1: mov >f=int64#11,<new_f=int64#17
# asm 2: mov >f=x10,<new_f=x16
mov x10,x16

# qhasm: fuv = f & 1048575
# asm 1: and >fuv=int64#17, <f=int64#11, #1048575
# asm 2: and >fuv=x16, <f=x10, #1048575
and x16, x10, #1048575

# qhasm: grs = g & 1048575
# asm 1: and >grs=int64#18, <g=int64#10, #1048575
# asm 2: and >grs=x17, <g=x9, #1048575
and x17, x9, #1048575

# qhasm: fuv -= 2p41
# asm 1: sub <fuv=int64#17,<fuv=int64#17,<2p41=int64#8
# asm 2: sub <fuv=x16,<fuv=x16,<2p41=x7
sub x16,x16,x7

# qhasm: grs -= 2p62
# asm 1: sub <grs=int64#18,<grs=int64#18,<2p62=int64#9
# asm 2: sub <grs=x17,<grs=x17,<2p62=x8
sub x17,x17,x8

# qhasm:     m1 = m - 1 
# asm 1: sub >m1=int64#19,<m=int64#14,#1
# asm 2: sub >m1=x18,<m=x13,#1
sub x18,x13,#1

# qhasm:     grs & 1
# asm 1: tst <grs=int64#18, #1
# asm 2: tst <grs=x17, #1
tst x17, #1

# qhasm:     ff = fuv if Z=0 else 0
# asm 1: csel >ff=int64#20, <fuv=int64#17, xzr, ne
# asm 2: csel >ff=x19, <fuv=x16, xzr, ne
csel x19, x16, xzr, ne

# qhasm:     m1 & (grs >>> 1)
# asm 1: tst <m1=int64#19, <grs=int64#18, ROR #1
# asm 2: tst <m1=x18, <grs=x17, ROR #1
tst x18, x17, ROR #1

# qhasm:     m = m1 if N=0 else -m
# asm 1: csneg >m=int64#14, <m1=int64#19, <m=int64#14, pl
# asm 2: csneg >m=x13, <m1=x18, <m=x13, pl
csneg x13, x18, x13, pl

# qhasm:     fuv = grs if N=1 else fuv
# asm 1: csel >fuv=int64#17, <grs=int64#18, <fuv=int64#17, mi
# asm 2: csel >fuv=x16, <grs=x17, <fuv=x16, mi
csel x16, x17, x16, mi

# qhasm:     ff = ff if N=0 else -ff
# asm 1: csneg >ff=int64#20, <ff=int64#20, <ff=int64#20, pl
# asm 2: csneg >ff=x19, <ff=x19, <ff=x19, pl
csneg x19, x19, x19, pl

# qhasm:     grs = grs + ff
# asm 1: add >grs=int64#18,<grs=int64#18,<ff=int64#20
# asm 2: add >grs=x17,<grs=x17,<ff=x19
add x17,x17,x19

# qhasm:     grs = grs signed>> 1
# asm 1: asr >grs=int64#18, <grs=int64#18, #1
# asm 2: asr >grs=x17, <grs=x17, #1
asr x17, x17, #1

# qhasm:     free m1

# qhasm:     free ff

# qhasm:     m1 = m - 1 
# asm 1: sub >m1=int64#19,<m=int64#14,#1
# asm 2: sub >m1=x18,<m=x13,#1
sub x18,x13,#1

# qhasm:     grs & 1
# asm 1: tst <grs=int64#18, #1
# asm 2: tst <grs=x17, #1
tst x17, #1

# qhasm:     ff = fuv if Z=0 else 0
# asm 1: csel >ff=int64#20, <fuv=int64#17, xzr, ne
# asm 2: csel >ff=x19, <fuv=x16, xzr, ne
csel x19, x16, xzr, ne

# qhasm:     m1 & (grs >>> 1)
# asm 1: tst <m1=int64#19, <grs=int64#18, ROR #1
# asm 2: tst <m1=x18, <grs=x17, ROR #1
tst x18, x17, ROR #1

# qhasm:     m = m1 if N=0 else -m
# asm 1: csneg >m=int64#14, <m1=int64#19, <m=int64#14, pl
# asm 2: csneg >m=x13, <m1=x18, <m=x13, pl
csneg x13, x18, x13, pl

# qhasm:     fuv = grs if N=1 else fuv
# asm 1: csel >fuv=int64#17, <grs=int64#18, <fuv=int64#17, mi
# asm 2: csel >fuv=x16, <grs=x17, <fuv=x16, mi
csel x16, x17, x16, mi

# qhasm:     ff = ff if N=0 else -ff
# asm 1: csneg >ff=int64#20, <ff=int64#20, <ff=int64#20, pl
# asm 2: csneg >ff=x19, <ff=x19, <ff=x19, pl
csneg x19, x19, x19, pl

# qhasm:     grs = grs + ff
# asm 1: add >grs=int64#18,<grs=int64#18,<ff=int64#20
# asm 2: add >grs=x17,<grs=x17,<ff=x19
add x17,x17,x19

# qhasm:     grs = grs signed>> 1
# asm 1: asr >grs=int64#18, <grs=int64#18, #1
# asm 2: asr >grs=x17, <grs=x17, #1
asr x17, x17, #1

# qhasm:     free m1

# qhasm:     free ff

# qhasm:     m1 = m - 1 
# asm 1: sub >m1=int64#19,<m=int64#14,#1
# asm 2: sub >m1=x18,<m=x13,#1
sub x18,x13,#1

# qhasm:     grs & 1
# asm 1: tst <grs=int64#18, #1
# asm 2: tst <grs=x17, #1
tst x17, #1

# qhasm:     ff = fuv if Z=0 else 0
# asm 1: csel >ff=int64#20, <fuv=int64#17, xzr, ne
# asm 2: csel >ff=x19, <fuv=x16, xzr, ne
csel x19, x16, xzr, ne

# qhasm:     m1 & (grs >>> 1)
# asm 1: tst <m1=int64#19, <grs=int64#18, ROR #1
# asm 2: tst <m1=x18, <grs=x17, ROR #1
tst x18, x17, ROR #1

# qhasm:     m = m1 if N=0 else -m
# asm 1: csneg >m=int64#14, <m1=int64#19, <m=int64#14, pl
# asm 2: csneg >m=x13, <m1=x18, <m=x13, pl
csneg x13, x18, x13, pl

# qhasm:     fuv = grs if N=1 else fuv
# asm 1: csel >fuv=int64#17, <grs=int64#18, <fuv=int64#17, mi
# asm 2: csel >fuv=x16, <grs=x17, <fuv=x16, mi
csel x16, x17, x16, mi

# qhasm:     ff = ff if N=0 else -ff
# asm 1: csneg >ff=int64#20, <ff=int64#20, <ff=int64#20, pl
# asm 2: csneg >ff=x19, <ff=x19, <ff=x19, pl
csneg x19, x19, x19, pl

# qhasm:     grs = grs + ff
# asm 1: add >grs=int64#18,<grs=int64#18,<ff=int64#20
# asm 2: add >grs=x17,<grs=x17,<ff=x19
add x17,x17,x19

# qhasm:     grs = grs signed>> 1
# asm 1: asr >grs=int64#18, <grs=int64#18, #1
# asm 2: asr >grs=x17, <grs=x17, #1
asr x17, x17, #1

# qhasm:     free m1

# qhasm:     free ff

# qhasm:     m1 = m - 1 
# asm 1: sub >m1=int64#19,<m=int64#14,#1
# asm 2: sub >m1=x18,<m=x13,#1
sub x18,x13,#1

# qhasm:     grs & 1
# asm 1: tst <grs=int64#18, #1
# asm 2: tst <grs=x17, #1
tst x17, #1

# qhasm:     ff = fuv if Z=0 else 0
# asm 1: csel >ff=int64#20, <fuv=int64#17, xzr, ne
# asm 2: csel >ff=x19, <fuv=x16, xzr, ne
csel x19, x16, xzr, ne

# qhasm:     m1 & (grs >>> 1)
# asm 1: tst <m1=int64#19, <grs=int64#18, ROR #1
# asm 2: tst <m1=x18, <grs=x17, ROR #1
tst x18, x17, ROR #1

# qhasm:     m = m1 if N=0 else -m
# asm 1: csneg >m=int64#14, <m1=int64#19, <m=int64#14, pl
# asm 2: csneg >m=x13, <m1=x18, <m=x13, pl
csneg x13, x18, x13, pl

# qhasm:     fuv = grs if N=1 else fuv
# asm 1: csel >fuv=int64#17, <grs=int64#18, <fuv=int64#17, mi
# asm 2: csel >fuv=x16, <grs=x17, <fuv=x16, mi
csel x16, x17, x16, mi

# qhasm:     ff = ff if N=0 else -ff
# asm 1: csneg >ff=int64#20, <ff=int64#20, <ff=int64#20, pl
# asm 2: csneg >ff=x19, <ff=x19, <ff=x19, pl
csneg x19, x19, x19, pl

# qhasm:     grs = grs + ff
# asm 1: add >grs=int64#18,<grs=int64#18,<ff=int64#20
# asm 2: add >grs=x17,<grs=x17,<ff=x19
add x17,x17,x19

# qhasm:     grs = grs signed>> 1
# asm 1: asr >grs=int64#18, <grs=int64#18, #1
# asm 2: asr >grs=x17, <grs=x17, #1
asr x17, x17, #1

# qhasm:     free m1

# qhasm:     free ff

# qhasm:     m1 = m - 1 
# asm 1: sub >m1=int64#19,<m=int64#14,#1
# asm 2: sub >m1=x18,<m=x13,#1
sub x18,x13,#1

# qhasm:     grs & 1
# asm 1: tst <grs=int64#18, #1
# asm 2: tst <grs=x17, #1
tst x17, #1

# qhasm:     ff = fuv if Z=0 else 0
# asm 1: csel >ff=int64#20, <fuv=int64#17, xzr, ne
# asm 2: csel >ff=x19, <fuv=x16, xzr, ne
csel x19, x16, xzr, ne

# qhasm:     m1 & (grs >>> 1)
# asm 1: tst <m1=int64#19, <grs=int64#18, ROR #1
# asm 2: tst <m1=x18, <grs=x17, ROR #1
tst x18, x17, ROR #1

# qhasm:     m = m1 if N=0 else -m
# asm 1: csneg >m=int64#14, <m1=int64#19, <m=int64#14, pl
# asm 2: csneg >m=x13, <m1=x18, <m=x13, pl
csneg x13, x18, x13, pl

# qhasm:     fuv = grs if N=1 else fuv
# asm 1: csel >fuv=int64#17, <grs=int64#18, <fuv=int64#17, mi
# asm 2: csel >fuv=x16, <grs=x17, <fuv=x16, mi
csel x16, x17, x16, mi

# qhasm:     ff = ff if N=0 else -ff
# asm 1: csneg >ff=int64#20, <ff=int64#20, <ff=int64#20, pl
# asm 2: csneg >ff=x19, <ff=x19, <ff=x19, pl
csneg x19, x19, x19, pl

# qhasm:     grs = grs + ff
# asm 1: add >grs=int64#18,<grs=int64#18,<ff=int64#20
# asm 2: add >grs=x17,<grs=x17,<ff=x19
add x17,x17,x19

# qhasm:     grs = grs signed>> 1
# asm 1: asr >grs=int64#18, <grs=int64#18, #1
# asm 2: asr >grs=x17, <grs=x17, #1
asr x17, x17, #1

# qhasm:     free m1

# qhasm:     free ff
# CHUNK